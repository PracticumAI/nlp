{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "# *Practicum AI:* NLP - Embeddings\n",
    "\n",
    "These exercises adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercises 4.01 - 4.06, page 159).\n",
    "\n",
    "(15 Minutes: Exercises 4.05 - 4.06)\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Word2Vec is a popular word embedding model, developed by Tomas Mikolov in 2013 at Google.  As such, it comes in two flavors (both shallow Neural Networks): Skip Gram and Common Bag Of Words (CBOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Our Own Embeddings - Page 197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> The code block below is in the text but is commented out as the text8 dataset is in the data folder.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of loading the data. if this doesn't work, you could use the text8 corpus local file\n",
    "# dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the data folder.\n",
    "dataset = word2vec.Text8Corpus(\"data/text8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reproducible results, set random seed to 1.\n",
    "\n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> The text does not import the numpy library so that needs to be done prior to setting the random seed.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.28147197e+00  4.93714213e-02 -1.77742636e+00  7.55727291e-01\n",
      " -4.48607475e-01 -1.61682606e+00  6.28471494e-01  1.29946005e+00\n",
      " -1.47697222e+00  7.33656824e-01 -2.50913620e-01  1.51851982e-01\n",
      " -8.39449286e-01  2.08915448e+00 -1.26336062e+00 -2.56653976e+00\n",
      "  5.14868915e-01  7.32167482e-01 -2.00053260e-01  5.38162589e-01\n",
      "  7.24335492e-01 -2.29516530e+00  1.24233079e+00  1.90678668e+00\n",
      "  2.64481211e+00 -3.21866304e-01 -2.15857774e-01  8.05656314e-01\n",
      "  7.09271729e-01 -3.06913555e-02  1.01351726e+00  1.23749733e+00\n",
      "  1.07249832e+00 -3.15461636e+00 -6.08641505e-01 -2.17098817e-01\n",
      "  1.12186813e+00  1.78734243e+00 -1.53762472e+00 -8.31309557e-02\n",
      "  5.96177101e-01 -7.12612689e-01 -6.29826248e-01  2.86765814e+00\n",
      " -1.24110842e+00  3.18110204e+00 -1.22431970e+00  7.18615949e-01\n",
      "  2.28058863e-02  1.10605106e-01 -3.84237826e-01 -1.09326792e+00\n",
      " -1.59575546e+00  1.34581447e+00  1.97974071e-01  1.38159752e+00\n",
      "  1.04777730e+00  1.09187233e+00 -6.55379534e-01 -4.87283587e-01\n",
      " -3.02258093e-04 -3.09790659e+00  3.12558860e-01 -7.55995929e-01\n",
      "  6.53878033e-01 -1.42505515e+00 -6.84361219e-01 -1.98722982e+00\n",
      "  2.03627542e-01 -2.62332344e+00 -1.22702301e+00 -7.83539638e-02\n",
      "  5.55929661e-01 -5.22257209e-01 -2.06089884e-01 -8.56916159e-02\n",
      "  1.27778232e-01  2.00050163e+00  8.79916787e-01 -3.12977016e-01\n",
      "  9.90636051e-01  7.68502578e-02  4.79292810e-01  2.22123456e+00\n",
      "  3.26232046e-01 -6.47940338e-01  5.29664755e-01 -8.46596301e-01\n",
      " -9.89370763e-01 -2.73317963e-01 -7.79004514e-01  9.23403680e-01\n",
      " -1.91273856e+00 -5.86819768e-01 -1.96264207e+00  9.31627750e-01\n",
      "  1.45146585e+00 -2.15020943e+00 -7.38836050e-01 -5.00052929e-01]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"animal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv[\"animal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('animals', 0.7239352464675903),\n",
       " ('insect', 0.7155537009239197),\n",
       " ('ants', 0.6823122501373291),\n",
       " ('insects', 0.679811954498291),\n",
       " ('aquatic', 0.660001814365387),\n",
       " ('organism', 0.656354546546936),\n",
       " ('eating', 0.6549410820007324),\n",
       " ('mammal', 0.6496347188949585),\n",
       " ('human', 0.6453425288200378),\n",
       " ('humans', 0.642937421798706)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humanity', 0.7880685329437256),\n",
       " ('goodness', 0.7656909227371216),\n",
       " ('compassion', 0.7424624562263489),\n",
       " ('dignity', 0.7383203506469727),\n",
       " ('fear', 0.7318388819694519),\n",
       " ('pleasure', 0.7316447496414185),\n",
       " ('perfection', 0.7248966693878174),\n",
       " ('salvation', 0.7177927494049072),\n",
       " ('mankind', 0.7145527601242065),\n",
       " ('immortality', 0.7055302262306213)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic Regularities in Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6914654970169067),\n",
       " ('princess', 0.6395758986473083),\n",
       " ('prince', 0.6220191717147827),\n",
       " ('daughter', 0.6123440265655518),\n",
       " ('empress', 0.6098198294639587)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wife', 0.8296380043029785),\n",
       " ('aunt', 0.8148874640464783),\n",
       " ('niece', 0.8077722787857056),\n",
       " ('daughter', 0.7944497466087341),\n",
       " ('grandmother', 0.7883173227310181)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['uncle', 'woman'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.05 (Vectors for Phrases) - Page 201\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extract the vector for 'get'\n",
    "\n",
    "```python\n",
    "v1 = model.wv['get']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract the vector for 'happy'\n",
    "\n",
    "```python\n",
    "v2 = model.wv['happy']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create a new vector as the average of the two vectors\n",
    "\n",
    "```python\n",
    "res1 = (v1 + v2) / 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Extract vectors for 'make' and 'merry'\n",
    "\n",
    "```python\n",
    "v1 = model.wv['make']\n",
    "v2 = model.wv['merry']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create a new vector as the average of the two vectors\n",
    "\n",
    "```python\n",
    "res2 = (v1+v2)/2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Find cosine similarity between the two averaged vectors\n",
    "\n",
    "```python\n",
    "model.wv.cosine_similarities(res1, [res2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Effect of Parameters - 'size' of the Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(dataset, size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('son', 0.8133434653282166),\n",
       " ('empress', 0.8022229671478271),\n",
       " ('emperor', 0.7999867796897888),\n",
       " ('archbishop', 0.7950774431228638),\n",
       " ('constantine', 0.7858606576919556)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of parameters - skipgram vs. CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rare terms - oeuvre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seminal', 0.7173739671707153),\n",
       " ('baglione', 0.6992780566215515),\n",
       " ('wace', 0.6952950954437256),\n",
       " ('mockery', 0.6938953399658203),\n",
       " ('foxe', 0.687375545501709)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"oeuvre\", topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = word2vec.Word2Vec(dataset, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('masterful', 0.8323545455932617),\n",
       " ('satiric', 0.8200669288635254),\n",
       " ('masterwork', 0.815832257270813),\n",
       " ('mussorgsky', 0.815514862537384),\n",
       " ('librettos', 0.8108195662498474)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(\"oeuvre\", topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.06 (Training Word Vectors on Different Datasets) - Page 205\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import the Brown and IMDb movie reviews corpus\n",
    "\n",
    "```python\n",
    "nltk.download('brown')\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "from nltk.corpus import brown, movie_reviews\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Extract the sentences and words using the .sent() method\n",
    "\n",
    "```python\n",
    "model_brown = word2vec.Word2Vec(brown.sents(), sg = 1)\n",
    "model_movie = word2vec.Word2Vec(movie_reviews.sents(), sg = 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Print the five terms most similar to 'money' in the Brown corpus\n",
    "\n",
    "```python\n",
    "model_brown.wv.most_similar('money', topn = 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Print the five terms most similar to 'money' in the movie corpus\n",
    "\n",
    "```python\n",
    "model_movie.wv.most_similar('money', topn = 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Using Pre-Trained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file     = 'data/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'data/glove.6B.100d.w2vformat.txt'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"data/glove.6B.100d.w2vformat.txt\", binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('funds', 0.8508071303367615),\n",
       " ('cash', 0.848483681678772),\n",
       " ('fund', 0.7594833374023438),\n",
       " ('paying', 0.7415367364883423),\n",
       " ('pay', 0.7407673001289368)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\"money\", topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755735874176025),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534753799438)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bias in Embeddings – A Word of Caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('child', 0.6149958372116089),\n",
       " ('nurse', 0.6090491414070129),\n",
       " ('teacher', 0.5878923535346985),\n",
       " ('dominatrix', 0.5384681224822998),\n",
       " ('detective', 0.5246642231941223)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'doctor'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pet', 0.6097452640533447),\n",
       " ('odie', 0.567996621131897),\n",
       " ('lingerie', 0.5643869042396545),\n",
       " ('scam', 0.5464061498641968),\n",
       " ('thug', 0.5415985584259033)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'smart'], negative=['man'], topn = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
