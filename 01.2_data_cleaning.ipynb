{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "These exercises adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercises 4.01 - 4.06, page 159).\n",
    "\n",
    "(15 Minutes: Exercises 4.01 - 4.03)\n",
    "\n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> All exercises for the NLP workshop series are included in this notebook as later exercises build on earlier ones.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:80% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:80% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Started with Text Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_txt = \"\"\"Welcome to the world of Deep Learning for NLP! We're in this together, and we'll learn together. \n",
    "NLP is amazing, and Deep Learning makes it even more fun. Let's learn!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/danielmaxwell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome to the world of Deep Learning for NLP!',\n",
       " \"We're in this together, and we'll learn together.\",\n",
       " 'NLP is amazing, and Deep Learning makes it even more fun.',\n",
       " \"Let's learn!\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.sent_tokenize(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_sents = tokenize.sent_tokenize(raw_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt_sents), len(txt_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]\n",
    "type(txt_words), type(txt_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Welcome', 'to', 'the', 'world', 'of', 'Deep', 'Learning', 'for', 'NLP', '!'], ['We', \"'re\", 'in', 'this', 'together', ',', 'and', 'we', \"'ll\", 'learn', 'together', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(txt_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You needn't run this\n",
    "raw_txt = raw_txt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome to the world of deep learning for nlp!',\n",
       " \"we're in this together, and we'll learn together.\",\n",
       " 'nlp is amazing, and deep learning makes it even more fun.',\n",
       " \"let's learn!\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_sents = [sent.lower() for sent in txt_sents]\n",
    "txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['welcome', 'to', 'the', 'world', 'of', 'deep', 'learning', 'for', 'nlp', '!'], ['we', \"'re\", 'in', 'this', 'together', ',', 'and', 'we', \"'ll\", 'learn', 'together', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(txt_words[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "list_punct = list(punctuation)\n",
    "print(list_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_punct(input_tokens):\n",
    "    return [token for token in input_tokens if token not in list_punct]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['let', 'us', 'go']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_punct([\"let\",\".\",\"us\",\".\",\"go\",\"!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['welcome', 'to', 'the', 'world', 'of', 'deep', 'learning', 'for', 'nlp'], ['we', \"'re\", 'in', 'this', 'together', 'and', 'we', \"'ll\", 'learn', 'together'], ['nlp', 'is', 'amazing', 'and', 'deep', 'learning', 'makes', 'it', 'even', 'more', 'fun'], ['let', \"'s\", 'learn']]\n"
     ]
    }
   ],
   "source": [
    "txt_words_nopunct = [drop_punct(sent) for sent in txt_words]\n",
    "print(txt_words_nopunct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/danielmaxwell/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "list_stop = stopwords.words(\"english\")\n",
    "len(list_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n"
     ]
    }
   ],
   "source": [
    "print(list_stop[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.01 (Tokenizing, Case Normalization, Punctuation and Stop Word...) - Page 166\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(1)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(2)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_txt = \"\"\"Welcome to the world of Deep Learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and Deep Learning makes it even more fun. Let's learn!\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(3)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(4)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(5)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(6)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(7)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(8)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stop(input_tokens):\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(9)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(10)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer_p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n"
     ]
    }
   ],
   "source": [
    "print(stemmer_p.stem(\"driving\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"I mustered all my drive, drove to the driving school!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'muster', 'all', 'my', 'drive', ',', 'drove', 'to', 'the', 'drive', 'school', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenize.word_tokenize(txt)\n",
    "print([stemmer_p.stem(word) for word in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rahim.baig\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pony'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"ponies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.02 (Stemming Our Data) - Page 172\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(1)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(2)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(3)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... applying stemmer to all the sentences\n",
    "\n",
    "<span style=\"color:steelblue\">**(4)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(5)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Downloading Text Corpora using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> The nltk.download() command, when executed without arguments, does not open the NLTK downloader in a new window, as pictured in the textbook.  The Unix version has a command line interface.  Type 'l' in the NLTK field and hit enter to view the Packages available for install.  As this list is rather long, you will need to hit enter multiple times to scroll through it.  The gutenberg package is in this list.  To install it, type 'd' and hit enter.  Then type 'gutenberg' in the NLTK field and hit enter again.  The software will respond with an installation message.  And finally, type 'q' to exit the downloader.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_raw = nltk.corpus.gutenberg.raw('carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_raw[:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representation\n",
    "##### One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcome', 'world', 'deep', 'learning', 'nlp'],\n",
       " [\"'re\", 'together', \"'ll\", 'learn', 'together'],\n",
       " ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],\n",
       " ['let', \"'s\", 'learn']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_words_nostop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.03 (Creating One-Hot Encoding for Our Data) - Page 181\n",
    "\n",
    "***\n",
    "\n",
    "<span style=\"color:steelblue\">**(1)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(2)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(3)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(4)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(5)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "##### Term Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=5, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(txt_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dtm = vectorizer.fit_transform(txt_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 2, 2],\n",
       "       [1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome to the world of deep learning for nlp!',\n",
       " \"we're in this together, and we'll learn together.\",\n",
       " 'nlp is amazing, and deep learning makes it even more fun.',\n",
       " \"let's learn!\"]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_nothing(doc):\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features = 5, \n",
    "                             preprocessor = do_nothing, \n",
    "                             tokenizer    = do_nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dtm = vectorizer.fit_transform(txt_words_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 0, 2],\n",
       "       [0, 1, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deep': 1, 'learn': 2, 'nlp': 3, 'togeth': 4, \"'ll\": 0}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['welcom', 'world', 'deep', 'learn', 'nlp'],\n",
       " [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n",
       " ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n",
       " ['let', \"'s\", 'learn']]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_words_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.04 (Document Term Matrix with TF-IDF) - Page 188\n",
    "\n",
    "***\n",
    "\n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> This exercise is optional.</div>\n",
    "\n",
    "<span style=\"color:steelblue\">**(1)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(2)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(3)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(4)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(5)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(6)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(7)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### NLP Workshop (2)\n",
    "#### Training Our Own Embeddings - Page 197"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> The code block below is in the text but is commented out as the text8 dataset is in the data folder.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of loading the data. if this doesn't work, you could use the text8 corpus local file\n",
    "# dataset = api.load(\"text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the data folder.\n",
    "dataset = word2vec.Text8Corpus(\"data/text8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure reproducible results, set random seed to 1.\n",
    "\n",
    "<div style=\"padding: 10px;margin-bottom: 20px;border: thin solid #30335D;border-left-width: 10px;background-color: #fff\"><strong>Note:</strong> The text does not import the numpy library so that needs to be done prior to setting the random seed.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4287323   0.63332057  1.2456409   1.4752632   2.109443    1.1569548\n",
      " -0.02130948  0.0182436  -0.31349027  0.54959863 -0.38544917 -0.6813727\n",
      "  0.6621224  -0.7377177  -1.359701    1.203772   -0.9285904   1.3970402\n",
      " -0.24282815  0.7036325   1.643011   -1.1232854  -0.8321323  -0.6999443\n",
      " -0.64125305  1.2003804  -2.5951238   0.9126282   0.40390974  0.34987825\n",
      " -0.6876566  -0.5630351  -1.4752318  -0.9742847  -1.2873613   0.364369\n",
      " -1.4539522  -0.68561345  3.5552766   0.72201556  1.5026729   0.98386985\n",
      " -0.28209195  1.3619014   1.5481789  -1.93158     0.2837808   1.1228682\n",
      " -0.26478738 -2.7449822   1.5130725   2.337826    0.990443    0.14571731\n",
      "  1.7722192   0.2289574   0.91638774 -0.9191398   0.31600663 -1.9637885\n",
      "  1.6257365   2.750085    0.82116085 -0.09666173 -1.0970197   1.7428256\n",
      " -1.2706696  -0.06126465  0.7258795  -2.1243627  -1.4855682   2.3246202\n",
      " -1.114259    1.3766924   0.03910503 -0.5288294  -0.11479293 -0.5253076\n",
      "  0.0630557  -0.49075633  0.02464008  0.9779139  -1.0765476  -0.9324076\n",
      "  0.09856791 -0.48495302 -0.02825055  0.24622656  0.04364256 -0.35242972\n",
      " -2.022148    2.8393102   1.4436     -0.8746849  -1.098391   -0.06354202\n",
      "  0.11641896  0.565676   -1.4897386  -1.9770645 ]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv[\"animal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv[\"animal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('insect', 0.7510448098182678),\n",
       " ('animals', 0.7423559427261353),\n",
       " ('aquatic', 0.6713050007820129),\n",
       " ('ants', 0.665005087852478),\n",
       " ('feces', 0.6597224473953247),\n",
       " ('humans', 0.6595240235328674),\n",
       " ('insects', 0.6531916856765747),\n",
       " ('domesticated', 0.639849066734314),\n",
       " ('mammal', 0.638308048248291),\n",
       " ('herd', 0.6336805820465088)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('humanity', 0.7809507846832275),\n",
       " ('goodness', 0.7767032384872437),\n",
       " ('pleasure', 0.7601181268692017),\n",
       " ('mankind', 0.733366072177887),\n",
       " ('satisfaction', 0.7316511869430542),\n",
       " ('compassion', 0.7286831736564636),\n",
       " ('desires', 0.7183192372322083),\n",
       " ('feelings', 0.7128568887710571),\n",
       " ('perfection', 0.7075753808021545),\n",
       " ('salvation', 0.7067484855651855)]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"happiness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Semantic Regularities in Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6851475238800049),\n",
       " ('prince', 0.6383140087127686),\n",
       " ('empress', 0.6180450916290283),\n",
       " ('princess', 0.6116641759872437),\n",
       " ('son', 0.6045335531234741)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aunt', 0.8399454951286316),\n",
       " ('grandmother', 0.8323032259941101),\n",
       " ('wife', 0.8157851099967957),\n",
       " ('niece', 0.8152226805686951),\n",
       " ('widow', 0.7859092950820923)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['uncle', 'woman'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.05 (Vectors for Phrases) - Page 201\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(1)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(2)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(3)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(4)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(5)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(6)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of Parameters - 'size' of the Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(dataset, size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('son', 0.8133434653282166),\n",
       " ('empress', 0.8022229671478271),\n",
       " ('emperor', 0.7999867796897888),\n",
       " ('archbishop', 0.7950774431228638),\n",
       " ('constantine', 0.7858606576919556)]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect of parameters - skipgram vs. CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Rare terms - oeuvre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('seminal', 0.7173739671707153),\n",
       " ('baglione', 0.6992780566215515),\n",
       " ('wace', 0.6952950954437256),\n",
       " ('mockery', 0.6938953399658203),\n",
       " ('foxe', 0.687375545501709)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"oeuvre\", topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = word2vec.Word2Vec(dataset, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('masterful', 0.8323545455932617),\n",
       " ('satiric', 0.8200669288635254),\n",
       " ('masterwork', 0.815832257270813),\n",
       " ('mussorgsky', 0.815514862537384),\n",
       " ('librettos', 0.8108195662498474)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sg.wv.most_similar(\"oeuvre\", topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4.06 (Training Word Vectors on Different Datasets) - Page 205\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(1)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(2)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(3)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:steelblue\">**(4)**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Pre-Trained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 100)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file     = 'data/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'data/glove.6B.100d.w2vformat.txt'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"data/glove.6B.100d.w2vformat.txt\", binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('funds', 0.8508071303367615),\n",
       " ('cash', 0.848483681678772),\n",
       " ('fund', 0.7594833374023438),\n",
       " ('paying', 0.7415367364883423),\n",
       " ('pay', 0.7407673001289368)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\"money\", topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7698541283607483),\n",
       " ('monarch', 0.6843380928039551),\n",
       " ('throne', 0.6755735874176025),\n",
       " ('daughter', 0.6594556570053101),\n",
       " ('princess', 0.6520534753799438)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(positive=['woman', 'king'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bias in Embeddings â€“ A Word of Caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('child', 0.6149958372116089),\n",
       " ('nurse', 0.6090491414070129),\n",
       " ('teacher', 0.5878923535346985),\n",
       " ('dominatrix', 0.5384681224822998),\n",
       " ('detective', 0.5246642231941223)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'doctor'], negative=['man'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pet', 0.6097452640533447),\n",
       " ('odie', 0.567996621131897),\n",
       " ('lingerie', 0.5643869042396545),\n",
       " ('scam', 0.5464061498641968),\n",
       " ('thug', 0.5415985584259033)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'smart'], negative=['man'], topn = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UFRC Python-3.8",
   "language": "python",
   "name": "python3-3.8-ufrc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
